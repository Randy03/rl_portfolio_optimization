{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6a8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import gym\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import random\n",
    "#from joblib import dump,load\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3fc58952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioBuffer():\n",
    "    def __init__(self,assets_names_list,assets_data_list,window):\n",
    "        #self.names = {0:'CASH'}\n",
    "        self.names = {}\n",
    "        for index,value in enumerate(assets_data_list):\n",
    "            self.names[index] = value\n",
    "        self.shape = assets_data_list[0].shape\n",
    "        for i in assets_data_list:\n",
    "            if self.shape != i.shape:\n",
    "                raise Exception('Data must be of the same size')\n",
    "        if len(assets_data_list) != len(assets_names_list):\n",
    "            raise Exception('The length of assets_names_list is different than the amount of assets in assets_data_list')\n",
    "        self.data = assets_data_list\n",
    "        self.shape = self.data.shape\n",
    "        self.pointer = window\n",
    "        self.window = window\n",
    "        self.batch_cache = None\n",
    "        self.length = self.shape[1]\n",
    "    \n",
    "    def get_batch(self):\n",
    "        if self.batch_cache is None:\n",
    "            batch = np.zeros(shape=(self.shape[0],self.window,self.shape[2]))\n",
    "            for index,data in enumerate(self.data):\n",
    "                batch[index] = data[self.pointer-self.window:self.pointer]/data[self.pointer-1][0]\n",
    "            self.batch_cache = batch\n",
    "        return self.batch_cache\n",
    "    \n",
    "    def get_next_batch(self):\n",
    "        self.pointer += 1\n",
    "        self.batch_cache = None\n",
    "        return self.get_batch()\n",
    "    \n",
    "    def get_current_price(self,index):\n",
    "        return self.data[index][self.pointer-1][0]\n",
    "    \n",
    "    def reset(self,position=None):\n",
    "        if not position:\n",
    "            self.pointer = random.randrange(self.window,self.length-self.window)\n",
    "        else:\n",
    "            self.pointer = self.window\n",
    "        self.batch_cache = None\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50ed6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnvironment(gym.Env):\n",
    "    def __init__(self,assets_names_list,assets_data_list,fee,initial_capital=100000,look_back_window=50,max_steps=200):\n",
    "        super(PortfolioEnvironment,self).__init__()\n",
    "        '''\n",
    "        assets_names_list: list with the ticker of each security\n",
    "        assets_data_list: list of pandas dataframes with the data of each security, must have the same length as assets_names_list and the first column of each dataframe must have the price of the asset\n",
    "        fee: porcentage of operating fee, with decimal, ie 0.1 is equal to 10% fee\n",
    "        initial_capital: amount of cash at the beginning\n",
    "        look_back_window: amount of periods to look back while executing a step\n",
    "        steps: maximum number of possible steps\n",
    "        '''\n",
    "        self.buffer = PortfolioBuffer(assets_names_list,np.array(list(map(lambda x: x.to_numpy(),assets_data_list))),look_back_window)\n",
    "        self.fee = fee\n",
    "        self.f = self.buffer.shape[2]\n",
    "        self.n = look_back_window\n",
    "        self.m = self.buffer.shape[0]\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        self.initial_capital = initial_capital\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=0.0,high=1.0,shape=(self.m+1,),dtype=np.float16)\n",
    "        #self.observation_space = gym.spaces.Box(low=0,high=1,shape=(self.f,self.n,self.m),dtype=np.float16)\n",
    "        self.observation_space = gym.spaces.Dict({\"data\": gym.spaces.Box(low=0,high=1,shape=(self.m,self.n,self.f),dtype=np.float16), \n",
    "                                              \"weights\": gym.spaces.Box(low=0.0,high=1.0,shape=(self.m+1,),dtype=np.float16)})\n",
    "        \n",
    "        #self.weights = np.resize(np.array([1.0]+[0.0]*(self.m-1)),(self.n,self.m))\n",
    "        self.weights = np.array([1.0]+[0.0]*(self.m))\n",
    "        self.portfolio_value = 1.0\n",
    "        \n",
    "    def _buy(self,index,price,amount):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _sell(self,index,price,amount):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _price_relative_vector(self):\n",
    "        '''\n",
    "        returns a matrix with the division of each assets value by the previous one\n",
    "        '''\n",
    "        prices = self.buffer.get_batch()[:,:,0].T\n",
    "        prices_diff = prices[:-1]/prices[1:]\n",
    "        prices_diff = np.concatenate((np.ones(shape=(prices_diff.shape[0],1)),prices_diff),axis=1)\n",
    "        return prices_diff\n",
    "        \n",
    "    def _weights_at_end_of_period(self):\n",
    "        '''\n",
    "        returns a vector with the weights of the portfolio after the new prices but before taking any action\n",
    "        '''\n",
    "        y = self._price_relative_vector()[-1]\n",
    "        return np.multiply(y,self.weights)/np.dot(y,self.weights)\n",
    "    \n",
    "    def _operation_cost(self,weights):\n",
    "        '''\n",
    "        weights: vector with the new weights provided by the actor\n",
    "        returns a scalar value with the cost of doing the buy/sell operations needed to get to those weights\n",
    "        '''\n",
    "        w_prime = self._weights_at_end_of_period()[1:]\n",
    "        return self.fee * np.sum(np.abs(weights[1:]-w_prime))\n",
    "    \n",
    "    def _portfolio_value_after_operation(self,weights):\n",
    "        '''\n",
    "        weights: vector with the new weights provided by the actor\n",
    "        returns a scalar with the new value of the portfolio after doing the buy/sell operations needed to get to those weights\n",
    "        '''\n",
    "        c = self._operation_cost(weights)\n",
    "        p0 = self.portfolio_value\n",
    "        y = self._price_relative_vector()[-1]\n",
    "        w = self.weights\n",
    "        return p0 * (1 - c) * np.dot(y, w)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        p1 = self._portfolio_value_after_operation(action)\n",
    "        \n",
    "        reward = np.log(p1/self.portfolio_value) / self.max_steps\n",
    "        \n",
    "        self.weights = action\n",
    "        \n",
    "        self.portfolio_value = p1\n",
    "        done = 0 if self.buffer.length-1 > self.buffer.pointer and self.current_step < self.max_steps and self.weights[0] > 0.0 else 1 \n",
    "        info = {}\n",
    "        self.current_step += 1\n",
    "        obs = {\"data\":self.buffer.get_next_batch(),\"weights\":self.weights}\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.weights = np.array([1.0]+[0.0]*(self.m))\n",
    "        self.portfolio_value = 1.0\n",
    "        self.current_step = 0\n",
    "        self.buffer.reset()\n",
    "        return {\"data\":self.buffer.get_batch(),\"weights\":self.weights}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91706a9",
   "metadata": {},
   "source": [
    "Experience replay, guarda las acciones realizadas por el actor los estados y las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57fe424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,max_size=1e6):\n",
    "        self.max_size = max_size\n",
    "        self.storage = [] # (s,s',a,r) memory\n",
    "        self.ptr = 0 #memory pointer\n",
    "    \n",
    "    def add(self, transition):\n",
    "        if len(self.storage)==self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr+1)%self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        ind = np.random.randint(0,len(self.storage),size=batch_size) \n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [],[],[],[],[]\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            batch_states.append(np.array(state,copy=False))\n",
    "            batch_next_states.append(np.array(next_state,copy=False))\n",
    "            batch_actions.append(np.array(action,copy=False))\n",
    "            batch_rewards.append(np.array(reward,copy=False))\n",
    "            batch_dones.append(np.array(done,copy=False))\n",
    "        return np.array(batch_states),np.array(batch_next_states),np.array(batch_actions),np.array(batch_rewards).reshape(-1,1),np.array(batch_dones).reshape(-1,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9e39681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self,state_dim,action_dim,max_action):\n",
    "        super(Actor,self).__init__()\n",
    "        self.layer_1 = tf.keras.layers.Dense(state_dim,activation='relu')\n",
    "        self.layer_2 = tf.keras.layers.Dense(400,activation='relu')\n",
    "        self.layer_3 = tf.keras.layers.Dense(300,activation='relu')\n",
    "        self.layer_4 = tf.keras.layers.Dense(action_dim,activation='softmax')\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def call(self, obs):\n",
    "        x = self.layer_1(obs)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = x * self.max_action\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a15cb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,state_dim,action_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.layer_1 = tf.keras.layers.Dense(state_dim+action_dim,activation='relu')\n",
    "        self.layer_2 = tf.keras.layers.Dense(400,activation='relu')\n",
    "        self.layer_3 = tf.keras.layers.Dense(300,activation='relu')\n",
    "        self.layer_4 = tf.keras.layers.Dense(1)\n",
    "        self.layer_5 = tf.keras.layers.Dense(state_dim+action_dim,activation='relu')\n",
    "        self.layer_6 = tf.keras.layers.Dense(400,activation='relu')\n",
    "        self.layer_7 = tf.keras.layers.Dense(300,activation='relu')\n",
    "        self.layer_8 = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, obs,actions):\n",
    "        x0 = tf.concat([obs, actions], 1)\n",
    "        x1 = self.layer_1(x0)\n",
    "        x1 = self.layer_2(x1)\n",
    "        x1 = self.layer_3(x1)\n",
    "        x1 = self.layer_4(x1)\n",
    "        \n",
    "        x2 = self.layer_5(x0)\n",
    "        x2 = self.layer_6(x2)\n",
    "        x2 = self.layer_7(x2)\n",
    "        x2 = self.layer_8(x2)\n",
    "        \n",
    "        return x1, x2\n",
    "        \n",
    "    def Q1(self, state, action):\n",
    "        x0 = tf.concat([state, action], 1)\n",
    "        x1 = self.layer_1(x0)\n",
    "        x1 = self.layer_2(x1)\n",
    "        x1 = self.layer_3(x1)\n",
    "        x1 = self.layer_4(x1)\n",
    "        return x1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e792d43b-416f-493e-9142-ebe95fc3748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3():\n",
    "    def __init__(self, state_dim, action_dim, max_action,lr=3e-4):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        for t, e in zip(self.actor_target.trainable_variables, self.actor.trainable_variables):\n",
    "            t.assign(e)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        for t, e in zip(self.critic_target.trainable_variables, self.critic.trainable_variables):\n",
    "            t.assign(e)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.critic_loss_fn = tf.keras.losses.Huber()\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        action = self.actor.call(state)[0].numpy()\n",
    "        return action\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clipping=0.5, policy_freq=2):\n",
    "        for i in range(iterations):\n",
    "            #get sample (s,s',a,r) from memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            #predict a' from s'\n",
    "            next_action = self.actor_target.call(batch_next_states)\n",
    "            \n",
    "            #add noise \n",
    "            #noise = tf.random.normal(next_action.shape, mean=0, stddev=policy_noise)\n",
    "            #noise = tf.clip_by_value(noise, -noise_clipping, noise_clipping)\n",
    "            #next_action = tf.clip_by_value(next_action + noise, 0, self.max_action)\n",
    "                        \n",
    "            target_Q1,target_Q2 = self.critic_target.call(batch_next_states,next_action)\n",
    "            #take minimum Q value\n",
    "            target_Q = tf.minimum(target_Q1,target_Q2)\n",
    "            #get final Q target, considering wether the episode has ended or not\n",
    "            target_Q = tf.stop_gradient(batch_rewards + (1 - batch_dones) * discount * target_Q)    \n",
    "            \n",
    "            #critic backpropagation\n",
    "            \n",
    "            trainable_critic_variables = self.critic.trainable_variables\n",
    "            \n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(trainable_critic_variables)\n",
    "                current_Q1, current_Q2 = self.critic.call(batch_states,batch_actions)\n",
    "                critic_loss = (self.critic_loss_fn(current_Q1,target_Q) + self.critic_loss_fn(current_Q2,target_Q))\n",
    "            critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n",
    "            self.critic_optimizer.apply_gradients(zip(critic_grads, trainable_critic_variables))\n",
    "                     \n",
    "            #AUpdate actor model\n",
    "            if i%policy_freq==0:\n",
    "                trainable_actor_variables = self.actor.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(trainable_actor_variables)\n",
    "                    #applying gradient ascent by taking de oposit function\n",
    "                    actor_loss = -tf.reduce_mean(self.critic.Q1(batch_states, self.actor(batch_states))) \n",
    "                actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n",
    "                self.actor_optimizer.apply_gradients(zip(actor_grads, trainable_actor_variables))\n",
    "            \n",
    "                # update the weights in the critic and actor target models, the tau parameter will define how much is going to adjust\n",
    "                for target_param, param in zip(self.critic_target.trainable_variables, self.critic.trainable_variables):\n",
    "                    target_param.assign(target_param * (1 - tau) + param * tau)\n",
    "                for target_param, param in zip(self.actor_target.trainable_variables, self.actor.trainable_variables):\n",
    "                    target_param.assign(target_param * (1 - tau) + param * tau)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def save(self):\n",
    "        time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.actor.save_weights(f'./models/{time}/actor')\n",
    "        self.actor_target.save_weights(f'./models/{time}/actor_target')\n",
    "        self.critic.save_weights(f'./models/{time}/critic')\n",
    "        self.critic_target.save_weights(f'./models/{time}/critic_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d4144c2-0f4b-4e47-b00c-3c89a5bc0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.0\n",
    "    for _ in range(eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print(f'Average Reward: {avg_reward}')\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26266508-cc6a-4b43-a616-ab3461ed48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "start_timesteps = 1e4 # Number of timesteps in which the model choose a random action, after that number starts using the policy\n",
    "eval_freq = 5e3 # policy evaluation frequency in timesteps\n",
    "max_timesteps = 5e5 \n",
    "save_models = True \n",
    "expl_noise = 0.1 # Exploration noise \n",
    "batch_size = 100 \n",
    "discount = 0.99 # reward Discount factor gamma \n",
    "tau = 0.005 # target weights update ratio\n",
    "policy_noise = 0.2 # std deviation of gaussian noise to be added to the action, for exploration purposes\n",
    "noise_clip = 0.5 # max value of gaussian noise added to action\n",
    "policy_freq = 2 # actor model weights update frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59ab36e5-87d5-45df-b025-5d0377c852ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = pd.read_csv('AAPL.csv')[['close','high','low']]\n",
    "xom = pd.read_csv('XOM.csv')[['close','high','low']]\n",
    "tsla = pd.read_csv('TSLA.csv')[['close','high','low']]\n",
    "assets_data_list = [aapl,xom,tsla]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "35ce7229-32ee-449c-9a65-aed6b0fbe473",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnvironment(['AAPL','XOM','TSLA'],assets_data_list,fee=0.025,look_back_window=40)\n",
    "env = gym.wrappers.FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c213e462-73a5-4da0-ad27-d8095fd88516",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d961e38-4c06-4a95-b9a0-83b8418574c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: -0.0001311608837627755\n"
     ]
    }
   ],
   "source": [
    "policy = TD3(state_dim,action_dim,max_action)\n",
    "np.seterr('raise')\n",
    "replay_buffer = ReplayBuffer()\n",
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69da470a-645a-4657-baaa-d27be6009783",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec426b-1aae-41c0-bcef-6ed4163bca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 201 Episode Num: 1 Reward: 0.649935428234472\n",
      "Total timesteps: 402 Episode Num: 2 Reward: 0.608969643320114\n",
      "Total timesteps: 603 Episode Num: 3 Reward: 0.6488049928618003\n",
      "Total timesteps: 804 Episode Num: 4 Reward: 0.625277419195717\n",
      "Total timesteps: 1005 Episode Num: 5 Reward: 0.6023878818387706\n",
      "Total timesteps: 1206 Episode Num: 6 Reward: 0.6004139843550615\n",
      "Total timesteps: 1345 Episode Num: 7 Reward: 0.4016809124241343\n",
      "Total timesteps: 1546 Episode Num: 8 Reward: 0.625277634770482\n",
      "Total timesteps: 1747 Episode Num: 9 Reward: 0.6583797616570931\n",
      "Total timesteps: 1948 Episode Num: 10 Reward: 0.6209790679867677\n",
      "Total timesteps: 2149 Episode Num: 11 Reward: 0.6007532760733452\n",
      "Total timesteps: 2350 Episode Num: 12 Reward: 0.6260874205596533\n",
      "Total timesteps: 2551 Episode Num: 13 Reward: 0.619657765141251\n",
      "Total timesteps: 2752 Episode Num: 14 Reward: 0.6077704013549624\n",
      "Total timesteps: 2953 Episode Num: 15 Reward: 0.6303671696568787\n",
      "Total timesteps: 3154 Episode Num: 16 Reward: 0.6384873735676414\n",
      "Total timesteps: 3355 Episode Num: 17 Reward: 0.5731185849714714\n",
      "Total timesteps: 3556 Episode Num: 18 Reward: 0.6478072149675771\n",
      "Total timesteps: 3757 Episode Num: 19 Reward: 0.5919512027163185\n",
      "Total timesteps: 3958 Episode Num: 20 Reward: 0.6239361204302822\n",
      "Total timesteps: 4115 Episode Num: 21 Reward: 0.48576066057483025\n",
      "Total timesteps: 4316 Episode Num: 22 Reward: 0.6194701500119196\n",
      "Total timesteps: 4517 Episode Num: 23 Reward: 0.616976252231238\n",
      "Total timesteps: 4718 Episode Num: 24 Reward: 0.6068733547078009\n",
      "Total timesteps: 4919 Episode Num: 25 Reward: 0.6365990996845017\n",
      "Total timesteps: 5120 Episode Num: 26 Reward: 0.647380671639849\n",
      "Average Reward: -3.4493672933439816e-09\n",
      "Total timesteps: 5321 Episode Num: 27 Reward: 0.6208186463859738\n",
      "Total timesteps: 5522 Episode Num: 28 Reward: 0.6231988829691549\n",
      "Total timesteps: 5723 Episode Num: 29 Reward: 0.5960596344979219\n",
      "Total timesteps: 5924 Episode Num: 30 Reward: 0.6260686879599843\n",
      "Total timesteps: 5968 Episode Num: 31 Reward: 0.127214155357141\n",
      "Total timesteps: 6169 Episode Num: 32 Reward: 0.5973083905231756\n",
      "Total timesteps: 6370 Episode Num: 33 Reward: 0.6639826892668081\n",
      "Total timesteps: 6571 Episode Num: 34 Reward: 0.6241642074363627\n",
      "Total timesteps: 6772 Episode Num: 35 Reward: 0.6016555832677679\n",
      "Total timesteps: 6958 Episode Num: 36 Reward: 0.5668550622313574\n",
      "Total timesteps: 7001 Episode Num: 37 Reward: 0.13715894664213582\n",
      "Total timesteps: 7202 Episode Num: 38 Reward: 0.6341119620503712\n",
      "Total timesteps: 7336 Episode Num: 39 Reward: 0.4216643762865186\n",
      "Total timesteps: 7537 Episode Num: 40 Reward: 0.5958806922146551\n",
      "Total timesteps: 7738 Episode Num: 41 Reward: 0.6397099783757388\n",
      "Total timesteps: 7939 Episode Num: 42 Reward: 0.6148382280757776\n",
      "Total timesteps: 8140 Episode Num: 43 Reward: 0.6362929826198739\n",
      "Total timesteps: 8341 Episode Num: 44 Reward: 0.6066088423798888\n",
      "Total timesteps: 8542 Episode Num: 45 Reward: 0.6173167938587998\n",
      "Total timesteps: 8608 Episode Num: 46 Reward: 0.2090014188637968\n",
      "Total timesteps: 8782 Episode Num: 47 Reward: 0.5410030797296498\n",
      "Total timesteps: 8983 Episode Num: 48 Reward: 0.6133490654572699\n",
      "Total timesteps: 9184 Episode Num: 49 Reward: 0.611552501761382\n",
      "Total timesteps: 9331 Episode Num: 50 Reward: 0.47470298830533425\n",
      "Total timesteps: 9532 Episode Num: 51 Reward: 0.6380723008693177\n",
      "Total timesteps: 9718 Episode Num: 52 Reward: 0.545084877389377\n",
      "Total timesteps: 9919 Episode Num: 53 Reward: 0.6024462444340724\n",
      "Total timesteps: 10120 Episode Num: 54 Reward: 0.24766058886953046\n",
      "Average Reward: -2.375247199420199e-08\n",
      "Total timesteps: 10297 Episode Num: 55 Reward: -3.977237776022167e-08\n",
      "Total timesteps: 10498 Episode Num: 56 Reward: -4.6553280460470427e-08\n",
      "Total timesteps: 10699 Episode Num: 57 Reward: -3.1254237759623286e-08\n",
      "Total timesteps: 10900 Episode Num: 58 Reward: -1.3977849358586663e-08\n",
      "Total timesteps: 11101 Episode Num: 59 Reward: -3.6754976176135113e-09\n",
      "Total timesteps: 11302 Episode Num: 60 Reward: 1.2222486974900263e-09\n",
      "Total timesteps: 11503 Episode Num: 61 Reward: -1.2225396177237936e-08\n",
      "Total timesteps: 11704 Episode Num: 62 Reward: -5.1946670320740853e-08\n",
      "Total timesteps: 11905 Episode Num: 63 Reward: 1.1443533688904563e-08\n",
      "Total timesteps: 12106 Episode Num: 64 Reward: 2.280207829170831e-08\n",
      "Total timesteps: 12307 Episode Num: 65 Reward: -1.613772441777081e-08\n",
      "Total timesteps: 12508 Episode Num: 66 Reward: 1.4170463962523682e-08\n",
      "Total timesteps: 12709 Episode Num: 67 Reward: 2.9816986999562277e-08\n",
      "Total timesteps: 12910 Episode Num: 68 Reward: -2.546434558103902e-08\n",
      "Total timesteps: 12996 Episode Num: 69 Reward: 3.449239400347432e-09\n",
      "Total timesteps: 13054 Episode Num: 70 Reward: 6.222253536623347e-09\n",
      "Total timesteps: 13255 Episode Num: 71 Reward: -1.3110188699974095e-09\n",
      "Total timesteps: 13456 Episode Num: 72 Reward: -1.1600738417304501e-08\n",
      "Total timesteps: 13657 Episode Num: 73 Reward: -1.2442390280924259e-08\n",
      "Total timesteps: 13858 Episode Num: 74 Reward: -4.999662038505586e-09\n",
      "Total timesteps: 14034 Episode Num: 75 Reward: 2.7495611679262458e-11\n",
      "Total timesteps: 14235 Episode Num: 76 Reward: -1.4601655062163343e-08\n",
      "Total timesteps: 14436 Episode Num: 77 Reward: -0.00013892582081366753\n",
      "Total timesteps: 14637 Episode Num: 78 Reward: -0.00015423975501586298\n",
      "Total timesteps: 14838 Episode Num: 79 Reward: -0.0001420487238960956\n",
      "Total timesteps: 14948 Episode Num: 80 Reward: -0.0001904217815282599\n",
      "Total timesteps: 15149 Episode Num: 81 Reward: -0.00010692876535527837\n",
      "Average Reward: -0.00010811557411811141\n",
      "Total timesteps: 15350 Episode Num: 82 Reward: -9.687257994164429e-05\n",
      "Total timesteps: 15531 Episode Num: 83 Reward: -0.00017249828080171463\n",
      "Total timesteps: 15690 Episode Num: 84 Reward: -0.00017015344439941775\n",
      "Total timesteps: 15891 Episode Num: 85 Reward: -0.00012404697225954455\n",
      "Total timesteps: 16092 Episode Num: 86 Reward: -0.00015564491022077324\n",
      "Total timesteps: 16293 Episode Num: 87 Reward: -0.00017904015420907633\n",
      "Total timesteps: 16494 Episode Num: 88 Reward: -4.7086499670227065e-05\n",
      "Total timesteps: 16695 Episode Num: 89 Reward: 5.784973227375977e-05\n",
      "Total timesteps: 16896 Episode Num: 90 Reward: -8.952992516351409e-05\n",
      "Total timesteps: 17097 Episode Num: 91 Reward: 2.6527475983968815e-05\n",
      "Total timesteps: 17298 Episode Num: 92 Reward: 8.516892577151668e-05\n",
      "Total timesteps: 17364 Episode Num: 93 Reward: -0.0001558251549752908\n",
      "Total timesteps: 17565 Episode Num: 94 Reward: -0.00012277280722370473\n",
      "Total timesteps: 17766 Episode Num: 95 Reward: -0.00012302810584802472\n",
      "Total timesteps: 17967 Episode Num: 96 Reward: -0.000109964714855736\n",
      "Total timesteps: 18142 Episode Num: 97 Reward: -0.00016363036004113767\n",
      "Total timesteps: 18343 Episode Num: 98 Reward: -0.0001235189219474997\n",
      "Total timesteps: 18467 Episode Num: 99 Reward: -0.0001445236377412356\n",
      "Total timesteps: 18544 Episode Num: 100 Reward: -0.00014745280076331694\n",
      "Total timesteps: 18719 Episode Num: 101 Reward: -0.00016362049640638356\n",
      "Total timesteps: 18920 Episode Num: 102 Reward: 2.024174504332703e-06\n",
      "Total timesteps: 19121 Episode Num: 103 Reward: -0.00022421509442166102\n",
      "Total timesteps: 19240 Episode Num: 104 Reward: -0.00014913570101373502\n",
      "Total timesteps: 19423 Episode Num: 105 Reward: -0.00016258279857266664\n",
      "Total timesteps: 19597 Episode Num: 106 Reward: -0.00015354872406442416\n",
      "Total timesteps: 19798 Episode Num: 107 Reward: -0.000134021061215685\n",
      "Total timesteps: 19959 Episode Num: 108 Reward: -0.00016342659780671027\n",
      "Total timesteps: 20148 Episode Num: 109 Reward: -0.00016208474013720695\n",
      "Average Reward: -0.00011067383210525044\n",
      "Total timesteps: 20349 Episode Num: 110 Reward: -0.00021149740910208543\n",
      "Total timesteps: 20550 Episode Num: 111 Reward: -0.00015550025266620102\n",
      "Total timesteps: 20751 Episode Num: 112 Reward: -0.00017133896812648915\n",
      "Total timesteps: 20794 Episode Num: 113 Reward: -0.00015845869582047835\n",
      "Total timesteps: 20995 Episode Num: 114 Reward: 5.1329732976116084e-06\n",
      "Total timesteps: 21196 Episode Num: 115 Reward: -0.00011360508308243863\n",
      "Total timesteps: 21397 Episode Num: 116 Reward: -8.732353444655058e-06\n",
      "Total timesteps: 21480 Episode Num: 117 Reward: -0.00017109906916711596\n",
      "Total timesteps: 21681 Episode Num: 118 Reward: -0.0001735302056623242\n",
      "Total timesteps: 21882 Episode Num: 119 Reward: -0.00013617899946478728\n",
      "Total timesteps: 22083 Episode Num: 120 Reward: -0.0001541325266160191\n",
      "Total timesteps: 22180 Episode Num: 121 Reward: -0.0001679749735002198\n",
      "Total timesteps: 22381 Episode Num: 122 Reward: -0.0001309674516311179\n",
      "Total timesteps: 22552 Episode Num: 123 Reward: -0.00018719232766443422\n",
      "Total timesteps: 22753 Episode Num: 124 Reward: -0.0001630270381538742\n",
      "Total timesteps: 22954 Episode Num: 125 Reward: -0.00012629711165664784\n",
      "Total timesteps: 23155 Episode Num: 126 Reward: -0.00014168689696601882\n",
      "Total timesteps: 23356 Episode Num: 127 Reward: -0.00016651348335001496\n",
      "Total timesteps: 23557 Episode Num: 128 Reward: -0.0001496097408223734\n",
      "Total timesteps: 23758 Episode Num: 129 Reward: -0.00018181884659858213\n",
      "Total timesteps: 23959 Episode Num: 130 Reward: -0.0001236751170355769\n",
      "Total timesteps: 24160 Episode Num: 131 Reward: -0.0001309542003174842\n",
      "Total timesteps: 24361 Episode Num: 132 Reward: -0.0001292069170930838\n",
      "Total timesteps: 24562 Episode Num: 133 Reward: -0.00016923631455151837\n",
      "Total timesteps: 24763 Episode Num: 134 Reward: -0.00013067519966362043\n",
      "Total timesteps: 24964 Episode Num: 135 Reward: -8.952264704733589e-05\n",
      "Total timesteps: 25160 Episode Num: 136 Reward: -0.00019127824896500518\n",
      "Average Reward: -0.00012317369027652742\n",
      "Total timesteps: 25361 Episode Num: 137 Reward: -0.0001490334766297448\n",
      "Total timesteps: 25562 Episode Num: 138 Reward: -0.00012833444362854167\n",
      "Total timesteps: 25763 Episode Num: 139 Reward: -0.00012804570409145622\n",
      "Total timesteps: 25964 Episode Num: 140 Reward: -8.427649341737023e-05\n",
      "Total timesteps: 26165 Episode Num: 141 Reward: -0.000151628398056912\n",
      "Total timesteps: 26366 Episode Num: 142 Reward: -0.0001572058434802647\n",
      "Total timesteps: 26482 Episode Num: 143 Reward: -0.00016419842856086353\n",
      "Total timesteps: 26683 Episode Num: 144 Reward: -0.00015225704627503924\n",
      "Total timesteps: 26884 Episode Num: 145 Reward: -0.00013329360286262575\n",
      "Total timesteps: 26983 Episode Num: 146 Reward: -0.00017320874927389334\n",
      "Total timesteps: 27184 Episode Num: 147 Reward: -0.00012629711165664784\n",
      "Total timesteps: 27385 Episode Num: 148 Reward: -0.00017712550254250263\n",
      "Total timesteps: 27586 Episode Num: 149 Reward: -0.0001616667311809781\n",
      "Total timesteps: 27787 Episode Num: 150 Reward: -0.00013241281313688928\n",
      "Total timesteps: 27860 Episode Num: 151 Reward: -0.0001650696858044889\n",
      "Total timesteps: 27904 Episode Num: 152 Reward: -0.0001525961493380289\n",
      "Total timesteps: 28105 Episode Num: 153 Reward: -0.00010034838249234897\n",
      "Total timesteps: 28306 Episode Num: 154 Reward: -9.508919815363961e-05\n",
      "Total timesteps: 28507 Episode Num: 155 Reward: -0.00010559897927699976\n",
      "Total timesteps: 28708 Episode Num: 156 Reward: -0.00017733029122210973\n",
      "Total timesteps: 28909 Episode Num: 157 Reward: -0.00014919777401483527\n",
      "Total timesteps: 28979 Episode Num: 158 Reward: -0.00016303698838822373\n",
      "Total timesteps: 29180 Episode Num: 159 Reward: -0.00015204928038978005\n",
      "Total timesteps: 29381 Episode Num: 160 Reward: -0.00016651348335001496\n",
      "Total timesteps: 29582 Episode Num: 161 Reward: -0.00015224656888020494\n",
      "Total timesteps: 29770 Episode Num: 162 Reward: -0.00019011050213035415\n",
      "Total timesteps: 29971 Episode Num: 163 Reward: -0.00014522839041084922\n",
      "Total timesteps: 30172 Episode Num: 164 Reward: -0.00018230962346472886\n",
      "Average Reward: -0.00016059571987286106\n",
      "Total timesteps: 30373 Episode Num: 165 Reward: -0.00014430256418954582\n",
      "Total timesteps: 30574 Episode Num: 166 Reward: -0.00013300752595786164\n",
      "Total timesteps: 30775 Episode Num: 167 Reward: -0.0001385302134190087\n",
      "Total timesteps: 30976 Episode Num: 168 Reward: -0.00015847875193068263\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "while total_timesteps < max_timesteps:\n",
    "    if done:\n",
    "        if total_timesteps != 0:\n",
    "            print(f'Total timesteps: {total_timesteps} Episode Num: {episode_num} Reward: {episode_reward}')\n",
    "            policy.train(replay_buffer,episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "        \n",
    "        if timesteps_since_eval >= eval_freq:\n",
    "            timesteps_since_eval %= eval_freq\n",
    "            evaluations.append(evaluate_policy(policy))\n",
    "            #policy.save(file_name)\n",
    "            #np.save(f'{file_name}_evaluation',evaluations)\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num +=1\n",
    "        \n",
    "    if total_timesteps < start_timesteps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = policy.select_action(np.array(obs))\n",
    "        #print(action)\n",
    "        #if expl_noise != 0: \n",
    "        #    action = (action + np.random.normal(0,expl_noise,size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "    \n",
    "    next_obs,reward,done,_ = env.step(action)\n",
    "    done_bool = 0 if episode_timesteps +1 == env.max_steps else float(done)\n",
    "    #done_bool = 0 if episode_timesteps +1 == env._max_episode_steps else float(done)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    replay_buffer.add((obs,next_obs,action,reward,done_bool))\n",
    "    \n",
    "    obs = next_obs\n",
    "    episode_timesteps +=1\n",
    "    total_timesteps +=1\n",
    "    timesteps_since_eval +=1\n",
    "\n",
    "print(evaluations.append(evaluate_policy(policy)))\n",
    "#policy.save(file_name)\n",
    "#np.save(f'{file_name}_evaluation',evaluations)\n",
    "print(f'Tiempo de entrenamiento: {time.time()-start_time} segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ac52d-847f-4700-b0d5-a1c322927033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
