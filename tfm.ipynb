{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6a8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import gym\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import random\n",
    "#from joblib import dump,load\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "#import pybullet_envs\n",
    "\n",
    "from environments import PortfolioEnvironment\n",
    "from utils import ReplayBuffer\n",
    "from policies import TD3,TD3p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4144c2-0f4b-4e47-b00c-3c89a5bc0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.0\n",
    "    for _ in range(eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            #action = tf.nn.softmax(action+ np.random.normal(0,1,size=4)).numpy()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "            #print(action)\n",
    "    avg_reward /= eval_episodes\n",
    "    print(f'Average Reward: {avg_reward}')\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26266508-cc6a-4b43-a616-ab3461ed48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "start_timesteps = 1e4 # Number of timesteps in which the model choose a random action, after that number starts using the policy\n",
    "eval_freq = 5e3 # policy evaluation frequency in timesteps\n",
    "max_timesteps = 5e5 \n",
    "save_models = True \n",
    "expl_noise = 0 # Exploration noise \n",
    "batch_size = 100 \n",
    "discount = 0.99 # reward Discount factor gamma \n",
    "tau = 0.005 # target weights update ratio\n",
    "policy_noise = 0.2 # std deviation of gaussian noise to be added to the action, for exploration purposes\n",
    "noise_clip = 0.5 # max value of gaussian noise added to action\n",
    "policy_freq = 2 # actor model weights update frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ab36e5-87d5-45df-b025-5d0377c852ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aapl = pd.read_csv('./data/AAPL.csv')[['close','high','low']]\n",
    "xom = pd.read_csv('./data/XOM.csv')[['close','high','low']]\n",
    "tsla = pd.read_csv('./data/TSLA.csv')[['close','high','low']]\n",
    "tickers = ['AAPL','XOM','TSLA']\n",
    "assets_data_list = [aapl,xom,tsla]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a00b461-af2f-4da2-a442-be8dbb65050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xom_daily = pd.read_csv('BTC-USD.csv')[['Adj Close','High','Low']]\n",
    "#tsla_daily = pd.read_csv('DOGE-USD.csv')[['Adj Close','High','Low']]\n",
    "xom_daily = pd.read_csv('./data/XOM_y.csv')[['Adj Close','High','Low']]\n",
    "tsla_daily = pd.read_csv('./data/TSLA_y.csv')[['Adj Close','High','Low']]\n",
    "aapl_daily = pd.read_csv('./data/AAPL_y.csv')[['Adj Close','High','Low']]\n",
    "tickers_daily = ['AAPL','XOM','TSLA']\n",
    "assets_data_list_daily = [aapl_daily,xom_daily,tsla_daily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35ce7229-32ee-449c-9a65-aed6b0fbe473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = PortfolioEnvironment(tickers_daily,assets_data_list_daily,fee=0.025,look_back_window=12,max_steps=24)\n",
    "env = PortfolioEnvironment(tickers,assets_data_list,fee=0.0025,look_back_window=40,max_steps=200)\n",
    "env = gym.wrappers.FlattenObservation(env)\n",
    "#env = gym.make(\"HalfCheetahBulletEnv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c213e462-73a5-4da0-ad27-d8095fd88516",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d961e38-4c06-4a95-b9a0-83b8418574c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 1.3806474577981047\n"
     ]
    }
   ],
   "source": [
    "policy = TD3p(state_dim,action_dim,max_action)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "evaluations = [evaluate_policy(policy,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69da470a-645a-4657-baaa-d27be6009783",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec426b-1aae-41c0-bcef-6ed4163bca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 201 Episode Num: 1 Reward: -0.001490716200161466 Portfolio Value:0.9985103943653187\n",
      "Total timesteps: 402 Episode Num: 2 Reward: -0.001585255285695405 Portfolio Value:0.9984160005677613\n",
      "Total timesteps: 603 Episode Num: 3 Reward: -0.001562334343522515 Portfolio Value:0.9984388854654455\n",
      "Total timesteps: 804 Episode Num: 4 Reward: -0.0016579010860431338 Portfolio Value:0.9983434724727827\n",
      "Total timesteps: 1005 Episode Num: 5 Reward: -0.0015642008919238943 Portfolio Value:0.9984370218326792\n",
      "Total timesteps: 1206 Episode Num: 6 Reward: -0.0015292208056728802 Portfolio Value:0.9984719478566733\n",
      "Total timesteps: 1407 Episode Num: 7 Reward: -0.0015052136305916937 Portfolio Value:0.9984959186352732\n",
      "Total timesteps: 1608 Episode Num: 8 Reward: -0.001736385796473157 Portfolio Value:0.9982651208491784\n",
      "Total timesteps: 1809 Episode Num: 9 Reward: -0.001553571885879434 Portfolio Value:0.998447634282219\n",
      "Total timesteps: 2010 Episode Num: 10 Reward: -0.0016793450727461239 Portfolio Value:0.9983220642381737\n",
      "Total timesteps: 2211 Episode Num: 11 Reward: -0.0016397345507393368 Portfolio Value:0.9983616090794598\n",
      "Total timesteps: 2412 Episode Num: 12 Reward: -0.0017042357524291473 Portfolio Value:0.9982972156327029\n",
      "Total timesteps: 2613 Episode Num: 13 Reward: -0.0014613723444770114 Portfolio Value:0.9985396949401242\n",
      "Total timesteps: 2814 Episode Num: 14 Reward: -0.001577042867550458 Portfolio Value:0.9984242000111091\n",
      "Total timesteps: 3015 Episode Num: 15 Reward: -0.001663291054573266 Portfolio Value:0.9983380914473856\n",
      "Total timesteps: 3216 Episode Num: 16 Reward: -0.001562105756229622 Portfolio Value:0.9984391136959134\n",
      "Total timesteps: 3417 Episode Num: 17 Reward: -0.001558536933463279 Portfolio Value:0.9984426769545116\n",
      "Total timesteps: 3618 Episode Num: 18 Reward: -0.0015989599282188504 Portfolio Value:0.9984023177271432\n",
      "Total timesteps: 3819 Episode Num: 19 Reward: -0.0017176556128169135 Portfolio Value:0.9982838187133362\n",
      "Total timesteps: 4020 Episode Num: 20 Reward: -0.0015417790624212468 Portfolio Value:0.9984594088683301\n",
      "Total timesteps: 4221 Episode Num: 21 Reward: -0.0015812403079990013 Portfolio Value:0.9984200091937825\n",
      "Total timesteps: 4263 Episode Num: 22 Reward: -0.0004305605485317032 Portfolio Value:0.9995695321293596\n",
      "Total timesteps: 4464 Episode Num: 23 Reward: -0.001754016333880265 Portfolio Value:0.9982475210537705\n",
      "Total timesteps: 4665 Episode Num: 24 Reward: -0.0017248048613514777 Portfolio Value:0.9982766817597202\n",
      "Total timesteps: 4866 Episode Num: 25 Reward: -0.001487461047394642 Portfolio Value:0.9985136446744818\n",
      "Total timesteps: 5067 Episode Num: 26 Reward: -0.0013301042145633841 Portfolio Value:0.9986707799819794\n",
      "Average Reward: 1.2950175670291713\n",
      "Total timesteps: 5268 Episode Num: 27 Reward: -0.001452181639063088 Portfolio Value:0.9985488722664775\n",
      "Total timesteps: 5427 Episode Num: 28 Reward: -0.001324159339718151 Portfolio Value:0.9986767169724253\n",
      "Total timesteps: 5628 Episode Num: 29 Reward: -0.0014505425551284731 Portfolio Value:0.9985505089732333\n",
      "Total timesteps: 5829 Episode Num: 30 Reward: -0.0015375403274242704 Portfolio Value:0.9984636410821391\n",
      "Total timesteps: 6030 Episode Num: 31 Reward: -0.001518244010151713 Portfolio Value:0.9984829079392317\n",
      "Total timesteps: 6231 Episode Num: 32 Reward: -0.001450174941350211 Portfolio Value:0.9985508760542262\n",
      "Total timesteps: 6432 Episode Num: 33 Reward: -0.0015695704673734303 Portfolio Value:0.9984316606641525\n",
      "Total timesteps: 6633 Episode Num: 34 Reward: -0.0015129248954643122 Portfolio Value:0.9984882189984574\n",
      "Total timesteps: 6834 Episode Num: 35 Reward: -0.0016057176508925968 Portfolio Value:0.99839557082396\n",
      "Total timesteps: 7035 Episode Num: 36 Reward: -0.0017177220818281745 Portfolio Value:0.9982837523584001\n",
      "Total timesteps: 7236 Episode Num: 37 Reward: -0.001508127213919066 Portfolio Value:0.9984930094384503\n",
      "Total timesteps: 7437 Episode Num: 38 Reward: -0.0015258568254096702 Portfolio Value:0.9984753067022487\n",
      "Total timesteps: 7638 Episode Num: 39 Reward: -0.0016442259688379123 Portfolio Value:0.9983571250301297\n",
      "Total timesteps: 7839 Episode Num: 40 Reward: -0.001455923376745624 Portfolio Value:0.9985451359655244\n",
      "Total timesteps: 8040 Episode Num: 41 Reward: -0.0015445968232742878 Portfolio Value:0.9984565954524581\n",
      "Total timesteps: 8217 Episode Num: 42 Reward: -0.001481977162753004 Portfolio Value:0.9985191204231364\n",
      "Total timesteps: 8418 Episode Num: 43 Reward: -0.0014976712766756352 Portfolio Value:0.9985034496732762\n",
      "Total timesteps: 8619 Episode Num: 44 Reward: -0.0015986913724342908 Portfolio Value:0.998402585853897\n",
      "Total timesteps: 8820 Episode Num: 45 Reward: -0.0015996738629121502 Portfolio Value:0.9984016049333452\n",
      "Total timesteps: 9021 Episode Num: 46 Reward: -0.0018090729133298494 Portfolio Value:0.9981925624727468\n",
      "Total timesteps: 9222 Episode Num: 47 Reward: -0.0015220728774615795 Portfolio Value:0.998479084887985\n",
      "Total timesteps: 9423 Episode Num: 48 Reward: -0.0016925202316633852 Portfolio Value:0.9983089112729732\n",
      "Total timesteps: 9624 Episode Num: 49 Reward: -0.0014991021909205786 Portfolio Value:0.9985020209014887\n",
      "Total timesteps: 9825 Episode Num: 50 Reward: -0.0017214792772611497 Portfolio Value:0.998280001618291\n",
      "Total timesteps: 10026 Episode Num: 51 Reward: 0.17125611038737815 Portfolio Value:1.1867946605428519\n",
      "Average Reward: 1.3806289586150142\n",
      "Total timesteps: 10227 Episode Num: 52 Reward: 1.3806559601687318 Portfolio Value:3.977509859991803\n",
      "Total timesteps: 10428 Episode Num: 53 Reward: 1.3806262863937182 Portfolio Value:3.97739183401025\n",
      "Total timesteps: 10629 Episode Num: 54 Reward: 1.3806522715044807 Portfolio Value:3.9774951883204333\n",
      "Total timesteps: 10830 Episode Num: 55 Reward: 1.3805906824163707 Portfolio Value:3.977250225562417\n",
      "Total timesteps: 11031 Episode Num: 56 Reward: 1.3806517022491938 Portfolio Value:3.9774929241109134\n",
      "Total timesteps: 11232 Episode Num: 57 Reward: 1.3806313954573612 Portfolio Value:3.9774121548101733\n",
      "Total timesteps: 11433 Episode Num: 58 Reward: 1.3806222479484505 Portfolio Value:3.977375771563454\n",
      "Total timesteps: 11536 Episode Num: 59 Reward: 0.7040885062663929 Portfolio Value:2.022002801822866\n",
      "Total timesteps: 11737 Episode Num: 60 Reward: 1.380640764727489 Portfolio Value:3.977449420433637\n",
      "Total timesteps: 11845 Episode Num: 61 Reward: 0.7385951680295203 Portfolio Value:2.092993144527149\n",
      "Total timesteps: 12046 Episode Num: 62 Reward: 1.3806485547125482 Portfolio Value:3.97748040482588\n",
      "Total timesteps: 12247 Episode Num: 63 Reward: 1.3806754416855234 Portfolio Value:3.977587348671725\n",
      "Total timesteps: 12448 Episode Num: 64 Reward: 1.380629026987536 Portfolio Value:3.9774027344406586\n",
      "Total timesteps: 12649 Episode Num: 65 Reward: 1.380655602460609 Portfolio Value:3.9775084372044716\n",
      "Total timesteps: 12809 Episode Num: 66 Reward: 1.097513925569017 Portfolio Value:2.9967067196426163\n",
      "Total timesteps: 13010 Episode Num: 67 Reward: 1.380666188145315 Portfolio Value:3.977550542077558\n",
      "Total timesteps: 13211 Episode Num: 68 Reward: 1.3806107392218763 Portfolio Value:3.9773299972966183\n",
      "Total timesteps: 13412 Episode Num: 69 Reward: 1.380582169910293 Portfolio Value:3.9772163693397995\n",
      "Total timesteps: 13613 Episode Num: 70 Reward: 1.3806283687121244 Portfolio Value:3.977400116215098\n",
      "Total timesteps: 13814 Episode Num: 71 Reward: 1.3805925853025953 Portfolio Value:3.9772577938242835\n",
      "Total timesteps: 14015 Episode Num: 72 Reward: 1.38066508131279 Portfolio Value:3.9775461395976848\n",
      "Total timesteps: 14216 Episode Num: 73 Reward: 1.380588354527613 Portfolio Value:3.9772409669771065\n",
      "Total timesteps: 14417 Episode Num: 74 Reward: 1.3806665007537224 Portfolio Value:3.9775517854934925\n",
      "Total timesteps: 14618 Episode Num: 75 Reward: 1.3806189425590336 Portfolio Value:3.977362624809399\n",
      "Total timesteps: 14819 Episode Num: 76 Reward: 1.380644315620564 Portfolio Value:3.977463543956316\n",
      "Total timesteps: 15020 Episode Num: 77 Reward: 1.380608442197799 Portfolio Value:3.9773208612843445\n",
      "Average Reward: 1.3806250119194519\n",
      "Total timesteps: 15221 Episode Num: 78 Reward: 1.3805882174088808 Portfolio Value:3.977240421622905\n",
      "Total timesteps: 15422 Episode Num: 79 Reward: 1.3806107392218763 Portfolio Value:3.9773299972966183\n",
      "Total timesteps: 15623 Episode Num: 80 Reward: 1.3805882174088808 Portfolio Value:3.977240421622905\n",
      "Total timesteps: 15824 Episode Num: 81 Reward: 1.380566834609815 Portfolio Value:3.9771553779993716\n",
      "Total timesteps: 16025 Episode Num: 82 Reward: 1.3806408379576633 Portfolio Value:3.977449711702962\n",
      "Total timesteps: 16226 Episode Num: 83 Reward: 1.380659028124805 Portfolio Value:3.9775220628360533\n",
      "Total timesteps: 16427 Episode Num: 84 Reward: 1.380613270966481 Portfolio Value:3.9773400668931265\n",
      "Total timesteps: 16628 Episode Num: 85 Reward: 1.380587511733537 Portfolio Value:3.9772376149833937\n",
      "Total timesteps: 16829 Episode Num: 86 Reward: 1.3806059689105523 Portfolio Value:3.9773110242395466\n",
      "Total timesteps: 17030 Episode Num: 87 Reward: 1.3806601522269488 Portfolio Value:3.977526533979644\n",
      "Total timesteps: 17231 Episode Num: 88 Reward: 1.380664903344084 Portfolio Value:3.977545431719008\n",
      "Total timesteps: 17432 Episode Num: 89 Reward: 1.380577758175415 Portfolio Value:3.9771988229543314\n",
      "Total timesteps: 17633 Episode Num: 90 Reward: 1.3806108871759228 Portfolio Value:3.9773305857587293\n",
      "Total timesteps: 17834 Episode Num: 91 Reward: 1.3805991774814284 Portfolio Value:3.9772840127053453\n",
      "Total timesteps: 18035 Episode Num: 92 Reward: 1.3805946901792194 Portfolio Value:3.9772661654700525\n",
      "Total timesteps: 18236 Episode Num: 93 Reward: 1.3806566389595476 Portfolio Value:3.977512559889882\n",
      "Total timesteps: 18437 Episode Num: 94 Reward: 1.380627640059534 Portfolio Value:3.977397218073256\n",
      "Total timesteps: 18638 Episode Num: 95 Reward: 1.3806043489345312 Portfolio Value:3.977304581096278\n",
      "Total timesteps: 18839 Episode Num: 96 Reward: 1.3806009805258666 Portfolio Value:3.9772911839316287\n",
      "Total timesteps: 19040 Episode Num: 97 Reward: 1.3805763121086485 Portfolio Value:3.977193071663448\n",
      "Total timesteps: 19241 Episode Num: 98 Reward: 1.380611271313125 Portfolio Value:3.9773321135996667\n",
      "Total timesteps: 19442 Episode Num: 99 Reward: 1.3805876911912547 Portfolio Value:3.9772383287294426\n",
      "Total timesteps: 19643 Episode Num: 100 Reward: 1.3806231594124538 Portfolio Value:3.97737939679995\n",
      "Total timesteps: 19844 Episode Num: 101 Reward: 1.3805684968916256 Portfolio Value:3.9771619891579095\n",
      "Total timesteps: 20045 Episode Num: 102 Reward: 1.3806189425590336 Portfolio Value:3.977362624809399\n",
      "Average Reward: 1.3806272952290237\n",
      "Total timesteps: 20202 Episode Num: 103 Reward: 1.0767996313541632 Portfolio Value:2.9352705551169267\n",
      "Total timesteps: 20403 Episode Num: 104 Reward: 1.380580366354605 Portfolio Value:3.977209196215063\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "info_list = []\n",
    "while total_timesteps < max_timesteps:\n",
    "    if done:\n",
    "        if total_timesteps != 0:\n",
    "            print(f'Total timesteps: {total_timesteps} Episode Num: {episode_num} Reward: {episode_reward} Portfolio Value:{np.exp(episode_reward)}')\n",
    "            policy.train(replay_buffer,episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "        \n",
    "        if timesteps_since_eval >= eval_freq:\n",
    "            timesteps_since_eval %= eval_freq\n",
    "            evaluations.append(evaluate_policy(policy))\n",
    "            #policy.save(file_name)\n",
    "            np.save(f'policy_evaluation',evaluations)\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num +=1\n",
    "        \n",
    "    if total_timesteps < start_timesteps:\n",
    "        action = tf.nn.softmax(env.action_space.sample()+ np.random.normal(0,1,size=env.action_space.shape[0])).numpy()\n",
    "    else:\n",
    "        action = policy.select_action(np.array(obs))\n",
    "        if expl_noise != 0: \n",
    "            #action = (action + np.random.normal(0,expl_noise,size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "            action = tf.nn.softmax((action + np.random.normal(0,expl_noise,size=env.action_space.shape[0]))).numpy()\n",
    "    \n",
    "    #print(action)\n",
    "    next_obs,reward,done, info = env.step(action)\n",
    "    info_list.append(info)\n",
    "    #done_bool = 0 if episode_timesteps +1 == env._max_episode_steps else float(done)\n",
    "    done_bool = 0 if episode_timesteps +1 == env.max_steps else float(done)\n",
    "    \n",
    "    #print(reward)\n",
    "    episode_reward += reward\n",
    "    replay_buffer.add((obs,next_obs,action,reward,done_bool))\n",
    "    \n",
    "    obs = next_obs\n",
    "    episode_timesteps +=1\n",
    "    total_timesteps +=1\n",
    "    timesteps_since_eval +=1\n",
    "\n",
    "print(evaluations.append(evaluate_policy(policy)))\n",
    "policy.save()\n",
    "np.save(f'policy_evaluation',evaluations)\n",
    "print(f'Tiempo de entrenamiento: {time.time()-start_time} segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23ae3d-7e94-46de-862f-b217d9e65220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc8208-ce86-4fdc-8983-48d5bf4d3cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd114f-f4d5-4462-a683-76d360156b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(env.action_space.sample() + np.random.normal(0,1,size=env.action_space.shape[0])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6eca2-c3f8-4a77-bd68-dfba93433c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c98dce-25a3-4c73-9e4f-5217d86af3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-0.00014784459103412868)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebbc5a-f60b-4e4a-b43c-460a9c6352f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.buffer.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c075c-cbb5-44d0-85ea-e5d8c8d7ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = tf.nn.softmax(env.action_space.sample())\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa445d6-ecd0-471c-ade8-2165ecba2a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24344839, 0.29504645, 0.0590641 , 0.40244105])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(np.random.normal(0,1,size=env.action_space.shape[0])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2a5e2b6-1593-4296-92e6-13e778fa2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.nn.softmax(np.random.normal(0,1,size=env.action_space.shape[0]-1)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "693db5ac-e33a-463d-a242-0de479c94506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165.14,  78.66, 875.84],\n",
       "       [165.53,  78.72, 864.9 ],\n",
       "       [165.68,  79.19, 864.59],\n",
       "       [165.51,  79.37, 861.35],\n",
       "       [165.75,  79.29, 863.52],\n",
       "       [165.96,  79.19, 867.56],\n",
       "       [166.22,  79.3 , 869.61],\n",
       "       [166.27,  79.42, 866.45],\n",
       "       [166.33,  79.54, 869.38],\n",
       "       [166.26,  79.62, 874.14],\n",
       "       [166.11,  79.69, 868.82],\n",
       "       [166.05,  79.8 , 866.31],\n",
       "       [165.9 ,  80.09, 862.84],\n",
       "       [165.77,  80.37, 864.16],\n",
       "       [165.71,  80.37, 862.99],\n",
       "       [165.83,  80.5 , 862.11],\n",
       "       [165.71,  80.47, 859.5 ],\n",
       "       [166.1 ,  80.35, 863.3 ],\n",
       "       [166.23,  80.48, 864.26],\n",
       "       [166.41,  80.6 , 865.16],\n",
       "       [166.23,  80.5 , 863.09],\n",
       "       [165.77,  80.36, 860.  ],\n",
       "       [165.75,  79.98, 859.39],\n",
       "       [165.6 ,  79.8 , 858.78],\n",
       "       [165.75,  80.06, 860.9 ],\n",
       "       [165.58,  79.94, 860.91],\n",
       "       [165.9 ,  79.97, 862.54],\n",
       "       [165.9 ,  80.05, 863.17],\n",
       "       [165.92,  79.98, 863.  ],\n",
       "       [165.74,  79.98, 860.84],\n",
       "       [166.  ,  80.17, 863.56],\n",
       "       [165.62,  80.16, 857.2 ],\n",
       "       [165.47,  80.22, 857.05],\n",
       "       [165.59,  80.07, 857.94],\n",
       "       [165.68,  79.99, 859.85],\n",
       "       [165.66,  79.9 , 861.45],\n",
       "       [165.59,  79.77, 861.31],\n",
       "       [165.56,  79.59, 861.97],\n",
       "       [165.63,  79.67, 862.32],\n",
       "       [165.52,  79.56, 861.24]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = env.buffer.data[:,:40,0].T\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da3409f7-7d18-4cdd-b74a-69ab58474bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.046992145312596836"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.dot(weights,data[-1])/np.dot(weights,data[-2]))*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "67f99a78-d329-4162-93e0-9e210ae913f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data/data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01863b56-0611-4f1a-8d1e-4d1e9ecb01f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04410925802795166"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.dot(weights,data[-1])/np.dot(weights,data[-2]))*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c622fa5-e1eb-48df-ab80-f3d9f48ebe39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9977042 , 0.98868778, 1.0169523 ],\n",
       "       [1.00006042, 0.98944193, 1.00424969],\n",
       "       [1.00096665, 0.99534942, 1.00388974],\n",
       "       [0.99993958, 0.99761187, 1.00012772],\n",
       "       [1.00138956, 0.99660633, 1.00264735],\n",
       "       [1.00265829, 0.99534942, 1.00733826],\n",
       "       [1.0042291 , 0.99673203, 1.00971855],\n",
       "       [1.00453117, 0.99824032, 1.00604942],\n",
       "       [1.00489367, 0.99974862, 1.00945149],\n",
       "       [1.00447076, 1.00075415, 1.0149784 ],\n",
       "       [1.00356452, 1.00163399, 1.00880126],\n",
       "       [1.00320203, 1.00301659, 1.00588686],\n",
       "       [1.0022958 , 1.00666164, 1.00185779],\n",
       "       [1.00151039, 1.010181  , 1.00339046],\n",
       "       [1.0011479 , 1.010181  , 1.00203195],\n",
       "       [1.00187289, 1.01181498, 1.00101017],\n",
       "       [1.0011479 , 1.01143791, 0.99797966],\n",
       "       [1.00350411, 1.00992961, 1.0023919 ],\n",
       "       [1.00428951, 1.0115636 , 1.00350657],\n",
       "       [1.00537699, 1.0130719 , 1.00455158],\n",
       "       [1.00428951, 1.01181498, 1.00214807],\n",
       "       [1.00151039, 1.0100553 , 0.99856022],\n",
       "       [1.00138956, 1.00527903, 0.99785193],\n",
       "       [1.00048333, 1.00301659, 0.99714365],\n",
       "       [1.00138956, 1.00628457, 0.99960522],\n",
       "       [1.00036249, 1.00477627, 0.99961683],\n",
       "       [1.0022958 , 1.00515334, 1.00150945],\n",
       "       [1.0022958 , 1.00615887, 1.00224095],\n",
       "       [1.00241663, 1.00527903, 1.00204357],\n",
       "       [1.00132914, 1.00527903, 0.99953555],\n",
       "       [1.00289995, 1.00766717, 1.00269379],\n",
       "       [1.00060416, 1.00754148, 0.99530909],\n",
       "       [0.99969792, 1.00829563, 0.99513492],\n",
       "       [1.00042291, 1.00641026, 0.99616832],\n",
       "       [1.00096665, 1.00540473, 0.99838605],\n",
       "       [1.00084582, 1.0042735 , 1.00024383],\n",
       "       [1.00042291, 1.00263952, 1.00008128],\n",
       "       [1.00024166, 1.00037707, 1.00084762],\n",
       "       [1.00066457, 1.0013826 , 1.00125401],\n",
       "       [1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77b6448d-2418-41d3-8842-0947538045e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.buffer.pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79a6016c-d573-4987-807a-f42e1d12d441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.00036153, 0.99925816, 0.99956936])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.buffer.get_price_relative_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c99a66c-ffce-4232-882e-a972d533c4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.00012087, 0.99886321, 0.99978199])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = np.zeros(shape=(env.buffer.shape[0],2,env.buffer.shape[2]))\n",
    "for index,data in enumerate(env.buffer.data):\n",
    "    batch[index] = data[env.buffer.pointer-2:env.buffer.pointer]\n",
    "prices = batch[:,:,0].T\n",
    "prices_diff = prices[1]/prices[0]\n",
    "prices_diff = np.concatenate((np.ones(shape=(1)),prices_diff))\n",
    "prices_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d66148c9-9e3b-4bd2-a03e-6b4082912799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[165.49, 165.6 , 165.42],\n",
       "       [ 79.08,  79.18,  79.05],\n",
       "       [871.31, 872.07, 870.6 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.buffer.data[:,107,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45463425-5212-45c8-ae8d-c426d5af5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9effc3d-348a-4624-8043-760ec8fe6c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe4be1f-687a-4a3e-a96a-d34a465620c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype)\n * (Tensor input, name dim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype)\n * (Tensor input, name dim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "torch.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e108d6f-1629-4f17-8f85-168e7cebc43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3811, 0.2359, 0.1225, 0.2605])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.Tensor(np.random.normal(0,1,size=4)),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de60e2c-3454-4fb9-a5bd-295bc9a21d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.3216, -0.6105,  1.0061,  0.5312])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5af19-8278-4074-818f-56e2b76c5352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl_gpu]",
   "language": "python",
   "name": "conda-env-rl_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
