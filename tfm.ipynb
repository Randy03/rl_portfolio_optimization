{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6a8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import gym\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import random\n",
    "#from joblib import dump,load\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fc58952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioBuffer():\n",
    "    def __init__(self,assets_names_list,assets_data_list,window):\n",
    "        #self.names = {0:'CASH'}\n",
    "        self.names = {}\n",
    "        for index,value in enumerate(assets_data_list):\n",
    "            self.names[index] = value\n",
    "        self.shape = assets_data_list[0].shape\n",
    "        for i in assets_data_list:\n",
    "            if self.shape != i.shape:\n",
    "                raise Exception('Data must be of the same size')\n",
    "        if len(assets_data_list) != len(assets_names_list):\n",
    "            raise Exception('The length of assets_names_list is different than the amount of assets in assets_data_list')\n",
    "        self.data = np.array([np.ones(shape=self.shape)] + assets_data_list)\n",
    "        self.shape = self.data.shape\n",
    "        self.pointer = window\n",
    "        self.window = window\n",
    "        self.batch_cache = None\n",
    "        self.length = self.shape[1]\n",
    "    \n",
    "    def get_batch(self):\n",
    "        if self.batch_cache is None:\n",
    "            batch = np.zeros(shape=(self.shape[0],self.window,self.shape[2]))\n",
    "            for index,data in enumerate(self.data):\n",
    "                batch[index] = data[self.pointer-self.window:self.pointer]/data[self.pointer-1][0]\n",
    "            self.batch_cache = batch\n",
    "        return self.batch_cache\n",
    "    \n",
    "    def get_next_batch(self):\n",
    "        self.pointer += 1\n",
    "        self.batch_cache = None\n",
    "        return self.get_batch()\n",
    "    \n",
    "    def get_current_price(self,index):\n",
    "        return self.data[index][pointer-1][0]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.pointer = self.window\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50ed6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioEnvironment(gym.Env):\n",
    "    def __init__(self,assets_names_list,assets_data_list,fee,initial_capital=100000,look_back_window=50,max_steps=200):\n",
    "        super(PortfolioEnvironment,self).__init__()\n",
    "        '''\n",
    "        assets_names_list: list with the ticker of each security\n",
    "        assets_data_list: list of pandas dataframes with the data of each security, must have the same length as assets_names_list and the first column of each dataframe must have the price of the asset\n",
    "        fee: porcentage of operating fee, with decimal, ie 0.1 is equal to 10% fee\n",
    "        initial_capital: amount of cash at the beginning\n",
    "        look_back_window: amount of periods to look back while executing a step\n",
    "        steps: maximum number of possible steps\n",
    "        '''\n",
    "        self.buffer = PortfolioBuffer(assets_names_list,np.array(list(map(lambda x: x.to_numpy(),assets_data_list))),look_back_window)\n",
    "        self.fee = fee\n",
    "        self.f = self.buffer.shape[2]\n",
    "        self.n = look_back_window\n",
    "        self.m = self.buffer.shape[0]\n",
    "        self.max_steps = max_steps\n",
    "        self.current_steps = 0\n",
    "        self.initial_capital = initial_capital\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=0.0,high=1.0,shape=(self.m+1,),dtype=np.float16)\n",
    "        #self.observation_space = gym.spaces.Box(low=0,high=1,shape=(self.f,self.n,self.m),dtype=np.float16)\n",
    "        self.observation_space = gym.spaces.Dict({\"data\": gym.spaces.Box(low=0,high=1,shape=(self.m,self.n,self.f),dtype=np.float16), \n",
    "                                              \"weights\": gym.spaces.Box(low=0.0,high=1.0,shape=(self.m+1,),dtype=np.float16)})\n",
    "        \n",
    "        #self.weights = np.resize(np.array([1.0]+[0.0]*(self.m-1)),(self.n,self.m))\n",
    "        self.weights = np.array([1.0]+[0.0]*(self.m))\n",
    "        self.portfolio_value = 1.0\n",
    "        \n",
    "    def _buy(self,index,price,amount):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _sell(self,index,price,amount):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _price_relative_vector(self):\n",
    "        '''\n",
    "        returns a matrix with the division of each assets value by the previous one\n",
    "        '''\n",
    "        prices = self.buffer.get_batch()[:,:,0].T\n",
    "        prices_diff = prices[:-1]/prices[1:]\n",
    "        prices_diff = np.concatenate((np.ones(shape=(prices_diff.shape[0],1)),prices_diff),axis=1)\n",
    "        return prices_diff\n",
    "        \n",
    "    def _weights_at_end_of_period(self):\n",
    "        '''\n",
    "        returns a vector with the weights of the portfolio after the new prices but before taking any action\n",
    "        '''\n",
    "        y = self._price_relative_vector()[-1]\n",
    "        return np.multiply(y,self.weights)/np.dot(y,self.weights)\n",
    "    \n",
    "    def _operation_cost(self,weights):\n",
    "        '''\n",
    "        weights: vector with the new weights provided by the actor\n",
    "        returns a scalar value with the cost of doing the buy/sell operations needed to get to those weights\n",
    "        '''\n",
    "        w_prime = self._weights_at_end_of_period()[1:]\n",
    "        return self.fee * np.sum(np.abs(weights[1:]-w_prime))\n",
    "    \n",
    "    def _portfolio_value_after_operation(self,weights):\n",
    "        '''\n",
    "        weights: vector with the new weights provided by the actor\n",
    "        returns a scalar with the new value of the portfolio after doing the buy/sell operations needed to get to those weights\n",
    "        '''\n",
    "        c = self._operation_cost(weights)\n",
    "        p0 = self.portfolio_value\n",
    "        y = self._price_relative_vector()[-1]\n",
    "        w = self.weights\n",
    "        return p0 * (1 - c) * np.dot(y, w)\n",
    "    \n",
    "    def _portfolio_return_rate(self):\n",
    "        mu = self._transaction_remainder_factor()\n",
    "        y = self._price_relative_vector()[-1]\n",
    "        w = self.weights\n",
    "        return np.dot(mu*y,w) - 1\n",
    "    \n",
    "    def _portfolio_log_return_rate(self):\n",
    "        mu = self._transaction_remainder_factor()\n",
    "        y = self._price_relative_vector()[-1]\n",
    "        w = self.weights\n",
    "        return np.log(np.dot(mu*y,w))\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        reward = -1\n",
    "        return reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        p1 = self._portfolio_value_after_operation(action)\n",
    "        self.weights = action\n",
    "        \n",
    "        \n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        self.portfolio_value = p1\n",
    "        done = 0 if self.buffer.length > self.current_step and self.current_step<self.max_steps else 1 \n",
    "        info = {}\n",
    "        self.current_step += 1\n",
    "        obs = {\"data\":self.buffer.get_next_batch(),\"weights\":self.weights}\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.weights = np.array([1.0]+[0.0]*(self.m))\n",
    "        self.portfolio_value = 1.0\n",
    "        self.buffer.reset()\n",
    "        return {\"data\":self.buffer.get_batch(),\"weights\":self.weights}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91706a9",
   "metadata": {},
   "source": [
    "Experience replay, guarda las acciones realizadas por el actor los estados y las recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57fe424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,max_size=1e6):\n",
    "        self.max_size = max_size\n",
    "        self.storage = [] #memoria para almacenar (s,s',a,r) \n",
    "        self.ptr = 0 #puntero de la memoria??\n",
    "    \n",
    "    def add(self, transition):\n",
    "        if len(self.storage)==self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr+1)%self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        ind = np.random.randint(0,len(self.storage),size=batch_size) #generar una cantidad batch_size de valores aleatorios\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [],[],[],[],[]\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            batch_states.append(np.array(state,copy=False))\n",
    "            batch_next_states.append(np.array(next_state,copy=False))\n",
    "            batch_actions.append(np.array(action,copy=False))\n",
    "            batch_rewards.append(np.array(reward,copy=False))\n",
    "            batch_dones.append(np.array(done,copy=False))\n",
    "        return np.array(batch_states),np.array(batch_next_states),np.array(batch_actions),np.array(batch_rewards).reshape(-1,1),np.array(batch_dones).reshape(-1,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e39681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self,state_dim,action_dim,max_action):\n",
    "        super(Actor,self).__init__()\n",
    "        self.layer_1 = tf.keras.layers.Dense(state_dim,activation='relu')\n",
    "        self.layer_2 = tf.keras.layers.Dense(400,activation='relu')\n",
    "        self.layer_3 = tf.keras.layers.Dense(300,activation='relu')\n",
    "        self.layer_4 = tf.keras.layers.Dense(action_dim,activation='softmax')\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def call(self, obs):\n",
    "        x = self.layer_1(obs)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = x * self.max_action\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a15cb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self,state_dim,action_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.layer_1 = tf.keras.layers.Dense(state_dim+action_dim,activation='relu')\n",
    "        self.layer_2 = tf.keras.layers.Dense(400,activation='relu')\n",
    "        self.layer_3 = tf.keras.layers.Dense(300,activation='relu')\n",
    "        self.layer_4 = tf.keras.layers.Dense(1)\n",
    "        self.layer_5 = tf.keras.layers.Dense(state_dim+action_dim,activation='relu')\n",
    "        self.layer_6 = tf.keras.layers.Dense(400,activation='relu')\n",
    "        self.layer_7 = tf.keras.layers.Dense(300,activation='relu')\n",
    "        self.layer_8 = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, obs,actions):\n",
    "        x0 = tf.concat([obs, actions], 1)\n",
    "        x1 = self.layer_1(x0)\n",
    "        x1 = self.layer_2(x1)\n",
    "        x1 = self.layer_3(x1)\n",
    "        x1 = self.layer_4(x1)\n",
    "        \n",
    "        x2 = self.layer_5(x0)\n",
    "        x2 = self.layer_6(x2)\n",
    "        x2 = self.layer_7(x2)\n",
    "        x2 = self.layer_8(x2)\n",
    "        \n",
    "        return x1, x2\n",
    "        \n",
    "    def Q1(self, state, action):\n",
    "        x0 = tf.concat([state, action], 1)\n",
    "        x1 = self.layer_1(x0)\n",
    "        x1 = self.layer_2(x1)\n",
    "        x1 = self.layer_3(x1)\n",
    "        x1 = self.layer_4(x1)\n",
    "        return x1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e792d43b-416f-493e-9142-ebe95fc3748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action,lr=3e-4):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        for t, e in zip(self.actor_target.trainable_variables, self.actor.trainable_variables):\n",
    "            t.assign(e)\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        for t, e in zip(self.critic_target.trainable_variables, self.critic.trainable_variables):\n",
    "            t.assign(e)\n",
    "        self.critic_optimizer = keras.optimizers.Adam(lr=actor_lr)\n",
    "        self.critic_loss_fn = tf.keras.losses.Huber()\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        action = self.actor.call(state)[0].numpy()\n",
    "        return action\n",
    "    \n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clipping=0.5, policy_freq=2):\n",
    "        for i in range(iterations):\n",
    "            #get sample (s,s',a,r) from memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            #predict a' from s'\n",
    "            next_action = self.actor_target.call(batch_next_states)\n",
    "            #add noise\n",
    "            noise = tf.random.normal(next_action.shape, mean=0, stddev=policy_noise)\n",
    "            noise = tf.clip_by_value(noise, -noise_clipping, noise_clipping)\n",
    "            next_action = tf.clip_by_value(next_action + noise, 0, self.max_action)\n",
    "                        \n",
    "            target_Q1,target_Q2 = self.critic_target.call(batch_next_states,next_action)\n",
    "            #take minimum Q value\n",
    "            target_Q = tf.minimum(target_Q1,target_Q2)\n",
    "            #get final Q target, considering wether the episode has ended or not\n",
    "            target_Q = tf.stop_gradient(batch_rewards + (1 - batch_dones) * discount * target_Q)    \n",
    "            \n",
    "            #critic backpropagation\n",
    "            \n",
    "            trainable_critic_variables = self.critic.trainable_variables\n",
    "            \n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(trainable_critic_variables)\n",
    "                current_Q1, current_Q2 = self.critic.call(batch_states,batch_actions)\n",
    "                critic_loss = (self.critic_loss_fn(current_Q1,target_Q) + self.critic_loss_fn(current_Q2,target_Q))\n",
    "            critic_grads = tape.gradient(critic_loss, trainable_critic_variables)\n",
    "            self.critic_optimizer.apply_gradients(zip(critic_grads, trainable_critic_variables))\n",
    "                     \n",
    "            #AUpdate actor model\n",
    "            if i%policy_freq==0:\n",
    "                trainable_actor_variables = self.actor.trainable_variables\n",
    "                with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                    tape.watch(trainable_actor_variables)\n",
    "                    #applying gradient ascent by taking de oposit function\n",
    "                    actor_loss = -tf.reduce_mean(self.critic.Q1(batch_states, self.actor(batch_states))) \n",
    "                actor_grads = tape.gradient(actor_loss, trainable_actor_variables)\n",
    "                self.actor_optimizer.apply_gradients(zip(actor_grads, trainable_actor_variables))\n",
    "            \n",
    "                # update the weights in the critic and actor target models, the tau parameter will define how much is going to adjust\n",
    "                for target_param, param in zip(self.critic_target.trainable_variables, self.critic.trainable_variables):\n",
    "                    target_param.assign(target_param * (1 - tau) + param * tau)\n",
    "                for target_param, param in zip(self.actor_target.trainable_variables, self.actor.trainable_variables):\n",
    "                    target_param.assign(target_param * (1 - tau) + param * tau)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def save(self, steps):\n",
    "        time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.actor.save_weights(f'./models/{time}/actor')\n",
    "        self.actor_target.save_weights(f'./models/{time}/actor_target')\n",
    "        self.critic.save_weights(f'./models/{time}/critic')\n",
    "        self.critic_target.save_weights(f'./models/{time}/critic_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d4144c2-0f4b-4e47-b00c-3c89a5bc0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.0\n",
    "    for _ in range(eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print(f'Recompensa promedio en la Evaluacion de la politica: {avg_reward}')\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26266508-cc6a-4b43-a616-ab3461ed48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 # Valor de la semilla aleatoria\n",
    "start_timesteps = 1e4 # Número de of iteraciones/timesteps durante las cuales el modelo elige una acción al azar, y después de las cuales comienza a usar la red de políticas\n",
    "eval_freq = 5e3 # Con qué frecuencia se realiza el paso de evaluación (después de cuántos pasos timesteps)\n",
    "max_timesteps = 5e5 # Número total de iteraciones/timesteps\n",
    "save_models = True # Check Boolean para saber si guardar o no el modelo pre-entrenado\n",
    "expl_noise = 0.1 # Ruido de exploración: desviación estándar del ruido de exploración gaussiano\n",
    "batch_size = 100 # Tamaño del bloque\n",
    "discount = 0.99 # Factor de descuento gamma, utilizado en el cáclulo de la recompensa de descuento total\n",
    "tau = 0.005 # Ratio de actualización de la red de objetivos\n",
    "policy_noise = 0.2 # Desviación estándar del ruido gaussiano añadido a las acciones para fines de exploración\n",
    "noise_clip = 0.5 # Valor máximo de ruido gaussiano añadido a las acciones (política)\n",
    "policy_freq = 2 # Número de iteraciones a esperar antes de actualizar la red de políticas (actor modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59ab36e5-87d5-45df-b025-5d0377c852ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "google = pd.read_csv('GOOG.csv')[['Adj Close','High','Low']]\n",
    "xom = pd.read_csv('XOM.csv')[['Adj Close','High','Low']]\n",
    "assets_data_list = [google,xom]\n",
    "env = PortfolioEnvironment(['asd','asd'],assets_data_list,fee=0.025,look_back_window=10)\n",
    "env2 = gym.wrappers.FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880323dd-7157-48dc-8410-f4f5cf1f3fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
